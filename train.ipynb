{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# K-Scale Humanoid Benchmark\n",
    "\n",
    "Welcome to the K-Scale Humanoid Benchmark! This notebook will walk you through training your own reinforcement learning policy, which you can then use to control a K-Scale robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Self\n",
    "\n",
    "import attrs\n",
    "import distrax\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import ksim\n",
    "import mujoco\n",
    "import mujoco_scenes\n",
    "import mujoco_scenes.mjcf\n",
    "import optax\n",
    "import xax\n",
    "from jaxtyping import Array, PRNGKeyArray\n",
    "from kscale.web.gen.api import JointMetadataOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_JOINTS = 20\n",
    "NUM_ACTOR_INPUTS = 43\n",
    "NUM_CRITIC_INPUTS = 444\n",
    "\n",
    "# These are in the order of the neural network outputs.\n",
    "ZEROS: list[tuple[str, float]] = [\n",
    "    (\"dof_right_shoulder_pitch_03\", 0.0),\n",
    "    (\"dof_right_shoulder_roll_03\", math.radians(-10.0)),\n",
    "    (\"dof_right_shoulder_yaw_02\", 0.0),\n",
    "    (\"dof_right_elbow_02\", math.radians(90.0)),\n",
    "    (\"dof_right_wrist_00\", 0.0),\n",
    "    (\"dof_left_shoulder_pitch_03\", 0.0),\n",
    "    (\"dof_left_shoulder_roll_03\", math.radians(10.0)),\n",
    "    (\"dof_left_shoulder_yaw_02\", 0.0),\n",
    "    (\"dof_left_elbow_02\", math.radians(-90.0)),\n",
    "    (\"dof_left_wrist_00\", 0.0),\n",
    "    (\"dof_right_hip_pitch_04\", math.radians(-25.0)),\n",
    "    (\"dof_right_hip_roll_03\", 0.0),\n",
    "    (\"dof_right_hip_yaw_03\", 0.0),\n",
    "    (\"dof_right_knee_04\", math.radians(-50.0)),\n",
    "    (\"dof_right_ankle_02\", math.radians(25.0)),\n",
    "    (\"dof_left_hip_pitch_04\", math.radians(25.0)),\n",
    "    (\"dof_left_hip_roll_03\", 0.0),\n",
    "    (\"dof_left_hip_yaw_03\", 0.0),\n",
    "    (\"dof_left_knee_04\", math.radians(50.0)),\n",
    "    (\"dof_left_ankle_02\", math.radians(-25.0)),\n",
    "]\n",
    "\n",
    "# These are the torques we clip outputs to when deploying the policy.\n",
    "MAX_TORQUE = {\n",
    "    \"00\": 1.0,  # 00 motor\n",
    "    \"02\": 13.0,  # 02 motor\n",
    "    \"03\": 48.0,  # 03 motor\n",
    "    \"04\": 96.0,  # 04 motor\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Rewards\n",
    "\n",
    "When training a reinforcement learning agent, the most important thing to define is what reward you want the agent to maximimze. `ksim` includes a number of useful default rewards for training walking agents, but it is often a good idea to define new rewards to encourage specific types of behavior. The cell below shows an example of how to define a custom reward. A similar pattern can be used to define custom objectives, events, observations, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@attrs.define\n",
    "class BentArmPenalty(ksim.Reward):\n",
    "    arm_indices: tuple[int, ...] = attrs.field()\n",
    "    arm_targets: tuple[float, ...] = attrs.field()\n",
    "\n",
    "    def get_reward(self, trajectory: ksim.Trajectory) -> Array:\n",
    "        qpos = trajectory.qpos[..., self.arm_indices]\n",
    "        qpos_targets = jnp.array(self.arm_targets)\n",
    "        qpos_diff = qpos - qpos_targets\n",
    "        return xax.get_norm(qpos_diff, \"l1\").mean(axis=-1)\n",
    "\n",
    "    @classmethod\n",
    "    def create(\n",
    "        cls,\n",
    "        model: ksim.PhysicsModel,\n",
    "        scale: float,\n",
    "        scale_by_curriculum: bool = False,\n",
    "    ) -> Self:\n",
    "        qpos_mapping = ksim.get_qpos_data_idxs_by_name(model)\n",
    "\n",
    "        names = [\n",
    "            \"dof_right_shoulder_pitch_03\",\n",
    "            \"dof_right_shoulder_roll_03\",\n",
    "            \"dof_right_shoulder_yaw_02\",\n",
    "            \"dof_right_elbow_02\",\n",
    "            \"dof_right_wrist_00\",\n",
    "            \"dof_left_shoulder_pitch_03\",\n",
    "            \"dof_left_shoulder_roll_03\",\n",
    "            \"dof_left_shoulder_yaw_02\",\n",
    "            \"dof_left_elbow_02\",\n",
    "            \"dof_left_wrist_00\",\n",
    "        ]\n",
    "\n",
    "        zeros = {k: v for k, v in ZEROS}\n",
    "        arm_indices = [qpos_mapping[name][0] for name in names]\n",
    "        arm_targets = [zeros[name] for name in names]\n",
    "\n",
    "        return cls(\n",
    "            arm_indices=tuple(arm_indices),\n",
    "            arm_targets=tuple(arm_targets),\n",
    "            scale=scale,\n",
    "            scale_by_curriculum=scale_by_curriculum,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Actor-Critic Model\n",
    "\n",
    "We train our reinforcement learning agent using an RNN-based actor and critic, which we define below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(eqx.Module):\n",
    "    \"\"\"Actor for the walking task.\"\"\"\n",
    "\n",
    "    input_proj: eqx.nn.Linear\n",
    "    rnns: tuple[eqx.nn.GRUCell, ...]\n",
    "    output_proj: eqx.nn.Linear\n",
    "    num_inputs: int = eqx.static_field()\n",
    "    num_outputs: int = eqx.static_field()\n",
    "    num_mixtures: int = eqx.static_field()\n",
    "    min_std: float = eqx.static_field()\n",
    "    max_std: float = eqx.static_field()\n",
    "    var_scale: float = eqx.static_field()\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        key: PRNGKeyArray,\n",
    "        *,\n",
    "        num_inputs: int,\n",
    "        num_outputs: int,\n",
    "        min_std: float,\n",
    "        max_std: float,\n",
    "        var_scale: float,\n",
    "        hidden_size: int,\n",
    "        num_mixtures: int,\n",
    "        depth: int,\n",
    "    ) -> None:\n",
    "        # Project input to hidden size\n",
    "        key, input_proj_key = jax.random.split(key)\n",
    "        self.input_proj = eqx.nn.Linear(\n",
    "            in_features=num_inputs,\n",
    "            out_features=hidden_size,\n",
    "            key=input_proj_key,\n",
    "        )\n",
    "\n",
    "        # Create RNN layer\n",
    "        key, rnn_key = jax.random.split(key)\n",
    "        self.rnns = tuple(\n",
    "            [\n",
    "                eqx.nn.GRUCell(\n",
    "                    input_size=hidden_size,\n",
    "                    hidden_size=hidden_size,\n",
    "                    key=rnn_key,\n",
    "                )\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Project to output\n",
    "        self.output_proj = eqx.nn.Linear(\n",
    "            in_features=hidden_size,\n",
    "            out_features=num_outputs * 3 * num_mixtures,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.num_mixtures = num_mixtures\n",
    "        self.min_std = min_std\n",
    "        self.max_std = max_std\n",
    "        self.var_scale = var_scale\n",
    "\n",
    "    def forward(self, obs_n: Array, carry: Array) -> tuple[distrax.Distribution, Array]:\n",
    "        x_n = self.input_proj(obs_n)\n",
    "        out_carries = []\n",
    "        for i, rnn in enumerate(self.rnns):\n",
    "            x_n = rnn(x_n, carry[i])\n",
    "            out_carries.append(x_n)\n",
    "        out_n = self.output_proj(x_n)\n",
    "\n",
    "        # Reshape the output to be a mixture of gaussians.\n",
    "        slice_len = NUM_JOINTS * self.num_mixtures\n",
    "        mean_nm = out_n[..., :slice_len].reshape(NUM_JOINTS, self.num_mixtures)\n",
    "        std_nm = out_n[..., slice_len : slice_len * 2].reshape(NUM_JOINTS, self.num_mixtures)\n",
    "        logits_nm = out_n[..., slice_len * 2 :].reshape(NUM_JOINTS, self.num_mixtures)\n",
    "\n",
    "        # Softplus and clip to ensure positive standard deviations.\n",
    "        std_nm = jnp.clip((jax.nn.softplus(std_nm) + self.min_std) * self.var_scale, max=self.max_std)\n",
    "\n",
    "        # Apply bias to the means.\n",
    "        mean_nm = mean_nm + jnp.array([v for _, v in ZEROS])[:, None]\n",
    "\n",
    "        dist_n = ksim.MixtureOfGaussians(means_nm=mean_nm, stds_nm=std_nm, logits_nm=logits_nm)\n",
    "\n",
    "        return dist_n, jnp.stack(out_carries, axis=0)\n",
    "\n",
    "\n",
    "class Critic(eqx.Module):\n",
    "    \"\"\"Critic for the walking task.\"\"\"\n",
    "\n",
    "    input_proj: eqx.nn.Linear\n",
    "    rnns: tuple[eqx.nn.GRUCell, ...]\n",
    "    output_proj: eqx.nn.Linear\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        key: PRNGKeyArray,\n",
    "        *,\n",
    "        hidden_size: int,\n",
    "        depth: int,\n",
    "    ) -> None:\n",
    "        num_inputs = NUM_CRITIC_INPUTS\n",
    "        num_outputs = 1\n",
    "\n",
    "        # Project input to hidden size\n",
    "        key, input_proj_key = jax.random.split(key)\n",
    "        self.input_proj = eqx.nn.Linear(\n",
    "            in_features=num_inputs,\n",
    "            out_features=hidden_size,\n",
    "            key=input_proj_key,\n",
    "        )\n",
    "\n",
    "        # Create RNN layer\n",
    "        key, rnn_key = jax.random.split(key)\n",
    "        self.rnns = tuple(\n",
    "            [\n",
    "                eqx.nn.GRUCell(\n",
    "                    input_size=hidden_size,\n",
    "                    hidden_size=hidden_size,\n",
    "                    key=rnn_key,\n",
    "                )\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Project to output\n",
    "        self.output_proj = eqx.nn.Linear(\n",
    "            in_features=hidden_size,\n",
    "            out_features=num_outputs,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    def forward(self, obs_n: Array, carry: Array) -> tuple[Array, Array]:\n",
    "        x_n = self.input_proj(obs_n)\n",
    "        out_carries = []\n",
    "        for i, rnn in enumerate(self.rnns):\n",
    "            x_n = rnn(x_n, carry[i])\n",
    "            out_carries.append(x_n)\n",
    "        out_n = self.output_proj(x_n)\n",
    "\n",
    "        return out_n, jnp.stack(out_carries, axis=0)\n",
    "\n",
    "\n",
    "class Model(eqx.Module):\n",
    "    actor: Actor\n",
    "    critic: Critic\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        key: PRNGKeyArray,\n",
    "        *,\n",
    "        num_inputs: int,\n",
    "        num_outputs: int,\n",
    "        min_std: float,\n",
    "        max_std: float,\n",
    "        hidden_size: int,\n",
    "        num_mixtures: int,\n",
    "        depth: int,\n",
    "    ) -> None:\n",
    "        self.actor = Actor(\n",
    "            key,\n",
    "            num_inputs=num_inputs,\n",
    "            num_outputs=num_outputs,\n",
    "            min_std=min_std,\n",
    "            max_std=max_std,\n",
    "            var_scale=0.5,\n",
    "            hidden_size=hidden_size,\n",
    "            num_mixtures=num_mixtures,\n",
    "            depth=depth,\n",
    "        )\n",
    "        self.critic = Critic(\n",
    "            key,\n",
    "            hidden_size=hidden_size,\n",
    "            depth=depth,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "The [ksim framework](https://github.com/kscalelabs/ksim) is based on [xax](https://github.com/kscalelabs/xax), a Jax training library built by K-Scale. To provide configuration options, Xax uses a Config dataclass to parse command-line options. We define the config here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HumanoidWalkingTaskConfig(ksim.PPOConfig):\n",
    "    \"\"\"Config for the humanoid walking task.\"\"\"\n",
    "\n",
    "    # Model parameters.\n",
    "    hidden_size: int = xax.field(\n",
    "        value=128,\n",
    "        help=\"The hidden size for the MLPs.\",\n",
    "    )\n",
    "    depth: int = xax.field(\n",
    "        value=5,\n",
    "        help=\"The depth for the MLPs.\",\n",
    "    )\n",
    "    num_mixtures: int = xax.field(\n",
    "        value=5,\n",
    "        help=\"The number of mixtures for the actor.\",\n",
    "    )\n",
    "    scale: float = xax.field(\n",
    "        value=0.1,\n",
    "        help=\"The maximum position delta on each step, in radians.\",\n",
    "    )\n",
    "\n",
    "    # Optimizer parameters.\n",
    "    learning_rate: float = xax.field(\n",
    "        value=3e-4,\n",
    "        help=\"Learning rate for PPO.\",\n",
    "    )\n",
    "    max_grad_norm: float = xax.field(\n",
    "        value=2.0,\n",
    "        help=\"Maximum gradient norm for clipping.\",\n",
    "    )\n",
    "    adam_weight_decay: float = xax.field(\n",
    "        value=1e-5,\n",
    "        help=\"Weight decay for the Adam optimizer.\",\n",
    "    )\n",
    "\n",
    "    # Curriculum parameters.\n",
    "    num_curriculum_levels: int = xax.field(\n",
    "        value=10,\n",
    "        help=\"The number of curriculum levels to use.\",\n",
    "    )\n",
    "    increase_threshold: float = xax.field(\n",
    "        value=3.0,\n",
    "        help=\"Increase the curriculum level when the mean trajectory length is above this threshold.\",\n",
    "    )\n",
    "    decrease_threshold: float = xax.field(\n",
    "        value=1.0,\n",
    "        help=\"Decrease the curriculum level when the mean trajectory length is below this threshold.\",\n",
    "    )\n",
    "    min_level_steps: int = xax.field(\n",
    "        value=50,\n",
    "        help=\"The minimum number of steps to wait before changing the curriculum level.\",\n",
    "    )\n",
    "    min_curriculum_level: float = xax.field(\n",
    "        value=0.0,\n",
    "        help=\"The minimum curriculum level to use.\",\n",
    "    )\n",
    "\n",
    "    # Rendering parameters.\n",
    "    render_track_body_id: int | None = xax.field(\n",
    "        value=0,\n",
    "        help=\"The body id to track with the render camera.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "The meat-and-potatoes of our training code is the task. This defines the observations, rewards, model calling logic, and everything else needed by `ksim` to train our reinforcement learning agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanoidWalkingTask(ksim.PPOTask[HumanoidWalkingTaskConfig]):\n",
    "    def get_optimizer(self) -> optax.GradientTransformation:\n",
    "        optimizer = optax.chain(\n",
    "            optax.clip_by_global_norm(self.config.max_grad_norm),\n",
    "            (\n",
    "                optax.adam(self.config.learning_rate)\n",
    "                if self.config.adam_weight_decay == 0.0\n",
    "                else optax.adamw(self.config.learning_rate, weight_decay=self.config.adam_weight_decay)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def get_mujoco_model(self) -> mujoco.MjModel:\n",
    "        mjcf_path = asyncio.run(ksim.get_mujoco_model_path(\"kbot-v2-feet\", name=\"robot\"))\n",
    "        return mujoco_scenes.mjcf.load_mjmodel(mjcf_path, scene=\"smooth\")\n",
    "\n",
    "    def get_mujoco_model_metadata(self, mj_model: mujoco.MjModel) -> dict[str, JointMetadataOutput]:\n",
    "        metadata = asyncio.run(ksim.get_mujoco_model_metadata(\"kbot-v2-feet\"))\n",
    "        if metadata.joint_name_to_metadata is None:\n",
    "            raise ValueError(\"Joint metadata is not available\")\n",
    "        return metadata.joint_name_to_metadata\n",
    "\n",
    "    def get_actuators(\n",
    "        self,\n",
    "        physics_model: ksim.PhysicsModel,\n",
    "        metadata: dict[str, JointMetadataOutput] | None = None,\n",
    "    ) -> ksim.Actuators:\n",
    "        assert metadata is not None, \"Metadata is required\"\n",
    "        return ksim.MITPositionActuators(\n",
    "            physics_model=physics_model,\n",
    "            joint_name_to_metadata=metadata,\n",
    "            ctrl_clip=[\n",
    "                # right arm\n",
    "                MAX_TORQUE[\"03\"],\n",
    "                MAX_TORQUE[\"03\"],\n",
    "                MAX_TORQUE[\"02\"],\n",
    "                MAX_TORQUE[\"02\"],\n",
    "                MAX_TORQUE[\"00\"],\n",
    "                # left arm\n",
    "                MAX_TORQUE[\"03\"],\n",
    "                MAX_TORQUE[\"03\"],\n",
    "                MAX_TORQUE[\"02\"],\n",
    "                MAX_TORQUE[\"02\"],\n",
    "                MAX_TORQUE[\"00\"],\n",
    "                # right leg\n",
    "                MAX_TORQUE[\"04\"],\n",
    "                MAX_TORQUE[\"03\"],\n",
    "                MAX_TORQUE[\"03\"],\n",
    "                MAX_TORQUE[\"04\"],\n",
    "                MAX_TORQUE[\"02\"],\n",
    "                # left leg\n",
    "                MAX_TORQUE[\"04\"],\n",
    "                MAX_TORQUE[\"03\"],\n",
    "                MAX_TORQUE[\"03\"],\n",
    "                MAX_TORQUE[\"04\"],\n",
    "                MAX_TORQUE[\"02\"],\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def get_physics_randomizers(self, physics_model: ksim.PhysicsModel) -> list[ksim.PhysicsRandomizer]:\n",
    "        return [\n",
    "            ksim.StaticFrictionRandomizer(),\n",
    "            ksim.FloorFrictionRandomizer.from_geom_name(physics_model, \"floor\", scale_lower=0.8, scale_upper=1.2),\n",
    "            ksim.ArmatureRandomizer(),\n",
    "            ksim.AllBodiesMassMultiplicationRandomizer(scale_lower=0.95, scale_upper=1.05),\n",
    "            ksim.JointDampingRandomizer(),\n",
    "            ksim.JointZeroPositionRandomizer(scale_lower=math.radians(-2), scale_upper=math.radians(2)),\n",
    "        ]\n",
    "\n",
    "    def get_events(self, physics_model: ksim.PhysicsModel) -> list[ksim.Event]:\n",
    "        return [\n",
    "            ksim.PushEvent(\n",
    "                x_force=1.5,\n",
    "                y_force=1.5,\n",
    "                z_force=0.1,\n",
    "                x_angular_force=0.1,\n",
    "                y_angular_force=0.1,\n",
    "                z_angular_force=0.3,\n",
    "                interval_range=(0.5, 4.0),\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    def get_resets(self, physics_model: ksim.PhysicsModel) -> list[ksim.Reset]:\n",
    "        return [\n",
    "            ksim.RandomJointPositionReset.create(physics_model, {k: v for k, v in ZEROS}, scale=0.1),\n",
    "            ksim.RandomJointVelocityReset(),\n",
    "        ]\n",
    "\n",
    "    def get_observations(self, physics_model: ksim.PhysicsModel) -> list[ksim.Observation]:\n",
    "        return [\n",
    "            ksim.JointPositionObservation(),\n",
    "            ksim.JointVelocityObservation(),\n",
    "            ksim.ActuatorForceObservation(),\n",
    "            ksim.CenterOfMassInertiaObservation(),\n",
    "            ksim.CenterOfMassVelocityObservation(),\n",
    "            ksim.BasePositionObservation(),\n",
    "            ksim.BaseOrientationObservation(),\n",
    "            ksim.BaseLinearVelocityObservation(),\n",
    "            ksim.BaseAngularVelocityObservation(),\n",
    "            ksim.BaseLinearAccelerationObservation(),\n",
    "            ksim.BaseAngularAccelerationObservation(),\n",
    "            ksim.ProjectedGravityObservation.create(\n",
    "                physics_model=physics_model,\n",
    "                framequat_name=\"base_link_quat\",\n",
    "                lag_range=(0.0, 0.5),\n",
    "            ),\n",
    "            ksim.ActuatorAccelerationObservation(),\n",
    "            ksim.BasePositionObservation(),\n",
    "            ksim.BaseOrientationObservation(),\n",
    "            ksim.BaseLinearVelocityObservation(),\n",
    "            ksim.BaseAngularVelocityObservation(),\n",
    "            ksim.CenterOfMassVelocityObservation(),\n",
    "            ksim.SensorObservation.create(physics_model=physics_model, sensor_name=\"imu_acc\"),\n",
    "            ksim.SensorObservation.create(physics_model=physics_model, sensor_name=\"imu_gyro\"),\n",
    "        ]\n",
    "\n",
    "    def get_commands(self, physics_model: ksim.PhysicsModel) -> list[ksim.Command]:\n",
    "        return []\n",
    "\n",
    "    def get_rewards(self, physics_model: ksim.PhysicsModel) -> list[ksim.Reward]:\n",
    "        return [\n",
    "            # Standard rewards.\n",
    "            ksim.StayAliveReward(scale=1.0),\n",
    "            ksim.NaiveForwardReward(clip_min=0.0, clip_max=0.5, scale=1.0),\n",
    "            ksim.UprightReward(index=\"x\", inverted=False, scale=0.1),\n",
    "            # Normalization penalties.\n",
    "            ksim.ActionInBoundsReward.create(physics_model, scale=0.01),\n",
    "            ksim.ActionSmoothnessPenalty(scale=-0.01),\n",
    "            ksim.ActuatorJerkPenalty(ctrl_dt=self.config.ctrl_dt, scale=-0.001),\n",
    "            ksim.ActuatorRelativeForcePenalty.create(physics_model, scale=-0.001),\n",
    "            ksim.AngularVelocityPenalty(index=\"x\", scale=-0.0005),\n",
    "            ksim.AngularVelocityPenalty(index=\"y\", scale=-0.0005),\n",
    "            ksim.AngularVelocityPenalty(index=\"z\", scale=-0.0005),\n",
    "            ksim.LinearVelocityPenalty(index=\"y\", scale=-0.0005),\n",
    "            ksim.LinearVelocityPenalty(index=\"z\", scale=-0.0005),\n",
    "            # Bespoke rewards.\n",
    "            BentArmPenalty.create(physics_model, scale=-0.01),\n",
    "        ]\n",
    "\n",
    "    def get_terminations(self, physics_model: ksim.PhysicsModel) -> list[ksim.Termination]:\n",
    "        return [\n",
    "            ksim.BadZTermination(unhealthy_z_lower=0.9, unhealthy_z_upper=1.6),\n",
    "            ksim.PitchTooGreatTermination(max_pitch=math.radians(30)),\n",
    "            ksim.RollTooGreatTermination(max_roll=math.radians(30)),\n",
    "            ksim.HighVelocityTermination(),\n",
    "            ksim.FarFromOriginTermination(max_dist=10.0),\n",
    "        ]\n",
    "\n",
    "    def get_curriculum(self, physics_model: ksim.PhysicsModel) -> ksim.Curriculum:\n",
    "        return ksim.EpisodeLengthCurriculum(\n",
    "            num_levels=self.config.num_curriculum_levels,\n",
    "            increase_threshold=self.config.increase_threshold,\n",
    "            decrease_threshold=self.config.decrease_threshold,\n",
    "            min_level_steps=self.config.min_level_steps,\n",
    "            dt=self.config.ctrl_dt,\n",
    "            min_level=self.config.min_curriculum_level,\n",
    "        )\n",
    "\n",
    "    def get_model(self, key: PRNGKeyArray) -> Model:\n",
    "        return Model(\n",
    "            key,\n",
    "            num_inputs=NUM_ACTOR_INPUTS,\n",
    "            num_outputs=NUM_JOINTS,\n",
    "            min_std=0.01,\n",
    "            max_std=1.0,\n",
    "            hidden_size=self.config.hidden_size,\n",
    "            num_mixtures=self.config.num_mixtures,\n",
    "            depth=self.config.depth,\n",
    "        )\n",
    "\n",
    "    def run_actor(\n",
    "        self,\n",
    "        model: Actor,\n",
    "        observations: xax.FrozenDict[str, Array],\n",
    "        commands: xax.FrozenDict[str, Array],\n",
    "        carry: Array,\n",
    "    ) -> tuple[distrax.Distribution, Array]:\n",
    "        joint_pos_n = observations[\"joint_position_observation\"]\n",
    "        joint_vel_n = observations[\"joint_velocity_observation\"]\n",
    "        proj_grav_3 = observations[\"projected_gravity_observation\"]\n",
    "\n",
    "        obs_n = jnp.concatenate(\n",
    "            [\n",
    "                joint_pos_n,  # NUM_JOINTS\n",
    "                joint_vel_n,  # NUM_JOINTS\n",
    "                proj_grav_3,  # 3\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "\n",
    "        action, carry = model.forward(obs_n, carry)\n",
    "\n",
    "        return action, carry\n",
    "\n",
    "    def run_critic(\n",
    "        self,\n",
    "        model: Critic,\n",
    "        observations: xax.FrozenDict[str, Array],\n",
    "        commands: xax.FrozenDict[str, Array],\n",
    "        carry: Array,\n",
    "    ) -> tuple[Array, Array]:\n",
    "        dh_joint_pos_j = observations[\"joint_position_observation\"]\n",
    "        dh_joint_vel_j = observations[\"joint_velocity_observation\"]\n",
    "        com_inertia_n = observations[\"center_of_mass_inertia_observation\"]\n",
    "        com_vel_n = observations[\"center_of_mass_velocity_observation\"]\n",
    "        imu_acc_3 = observations[\"sensor_observation_imu_acc\"]\n",
    "        imu_gyro_3 = observations[\"sensor_observation_imu_gyro\"]\n",
    "        proj_grav_3 = observations[\"projected_gravity_observation\"]\n",
    "        act_frc_obs_n = observations[\"actuator_force_observation\"]\n",
    "        base_pos_3 = observations[\"base_position_observation\"]\n",
    "        base_quat_4 = observations[\"base_orientation_observation\"]\n",
    "\n",
    "        obs_n = jnp.concatenate(\n",
    "            [\n",
    "                dh_joint_pos_j,  # NUM_JOINTS\n",
    "                dh_joint_vel_j / 10.0,  # NUM_JOINTS\n",
    "                com_inertia_n,  # 160\n",
    "                com_vel_n,  # 96\n",
    "                imu_acc_3,  # 3\n",
    "                imu_gyro_3,  # 3\n",
    "                proj_grav_3,  # 3\n",
    "                act_frc_obs_n / 100.0,  # NUM_JOINTS\n",
    "                base_pos_3,  # 3\n",
    "                base_quat_4,  # 4\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "\n",
    "        return model.forward(obs_n, carry)\n",
    "\n",
    "    def get_ppo_variables(\n",
    "        self,\n",
    "        model: Model,\n",
    "        trajectory: ksim.Trajectory,\n",
    "        model_carry: tuple[Array, Array],\n",
    "        rng: PRNGKeyArray,\n",
    "    ) -> tuple[ksim.PPOVariables, tuple[Array, Array]]:\n",
    "        def scan_fn(\n",
    "            actor_critic_carry: tuple[Array, Array],\n",
    "            transition: ksim.Trajectory,\n",
    "        ) -> tuple[tuple[Array, Array], ksim.PPOVariables]:\n",
    "            actor_carry, critic_carry = actor_critic_carry\n",
    "            actor_dist, next_actor_carry = self.run_actor(\n",
    "                model=model.actor,\n",
    "                observations=transition.obs,\n",
    "                commands=transition.command,\n",
    "                carry=actor_carry,\n",
    "            )\n",
    "            log_probs = actor_dist.log_prob(transition.action)\n",
    "            assert isinstance(log_probs, Array)\n",
    "            value, next_critic_carry = self.run_critic(\n",
    "                model=model.critic,\n",
    "                observations=transition.obs,\n",
    "                commands=transition.command,\n",
    "                carry=critic_carry,\n",
    "            )\n",
    "\n",
    "            transition_ppo_variables = ksim.PPOVariables(\n",
    "                log_probs=log_probs,\n",
    "                values=value.squeeze(-1),\n",
    "            )\n",
    "\n",
    "            next_carry = jax.tree.map(\n",
    "                lambda x, y: jnp.where(transition.done, x, y),\n",
    "                self.get_initial_model_carry(rng),\n",
    "                (next_actor_carry, next_critic_carry),\n",
    "            )\n",
    "\n",
    "            return next_carry, transition_ppo_variables\n",
    "\n",
    "        next_model_carry, ppo_variables = jax.lax.scan(scan_fn, model_carry, trajectory)\n",
    "\n",
    "        return ppo_variables, next_model_carry\n",
    "\n",
    "    def get_initial_model_carry(self, rng: PRNGKeyArray) -> tuple[Array, Array]:\n",
    "        return (\n",
    "            jnp.zeros(shape=(self.config.depth, self.config.hidden_size)),\n",
    "            jnp.zeros(shape=(self.config.depth, self.config.hidden_size)),\n",
    "        )\n",
    "\n",
    "    def sample_action(\n",
    "        self,\n",
    "        model: Model,\n",
    "        model_carry: tuple[Array, Array],\n",
    "        physics_model: ksim.PhysicsModel,\n",
    "        physics_state: ksim.PhysicsState,\n",
    "        observations: xax.FrozenDict[str, Array],\n",
    "        commands: xax.FrozenDict[str, Array],\n",
    "        rng: PRNGKeyArray,\n",
    "        argmax: bool,\n",
    "    ) -> ksim.Action:\n",
    "        actor_carry_in, critic_carry_in = model_carry\n",
    "\n",
    "        # Runs the actor model to get the action distribution.\n",
    "        action_dist_j, actor_carry = self.run_actor(\n",
    "            model=model.actor,\n",
    "            observations=observations,\n",
    "            commands=commands,\n",
    "            carry=actor_carry_in,\n",
    "        )\n",
    "\n",
    "        action_j = action_dist_j.mode() if argmax else action_dist_j.sample(seed=rng)\n",
    "\n",
    "        return ksim.Action(\n",
    "            action=action_j,\n",
    "            carry=(actor_carry, critic_carry_in),\n",
    "            aux_outputs=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Launching an Experiment\n",
    "\n",
    "To launch an experiment with `xax`, you can use `Task.launch(config)`. Note that this is usually intended to be called from the command-line, so it will by default attempt to parse additional command-line arguments unless `use_cli=False` is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "HumanoidWalkingTask.launch(\n",
    "    HumanoidWalkingTaskConfig(\n",
    "        # Training parameters.\n",
    "        num_envs=2048,\n",
    "        batch_size=256,\n",
    "        num_passes=2,\n",
    "        epochs_per_log_step=1,\n",
    "        rollout_length_seconds=8.0,\n",
    "        # Simulation parameters.\n",
    "        dt=0.002,\n",
    "        ctrl_dt=0.02,\n",
    "        iterations=8,\n",
    "        ls_iterations=8,\n",
    "        max_action_latency=0.01,\n",
    "        # Checkpointing parameters.\n",
    "        save_every_n_seconds=60,\n",
    "    ),\n",
    "    use_cli=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
